{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "path = {\n",
    "    'facebook':{\n",
    "        'post':'..\\\\data\\\\facebook\\\\fb_news_posts_20K.csv',\n",
    "        'comment':'..\\\\data\\\\facebook\\\\fb_news_comments_1000K_hashed.csv',\n",
    "        'source':'..\\\\data\\\\facebook\\\\fb_news_pagenames.csv'\n",
    "        },\n",
    "    'twitter':glob.glob('..\\\\data\\\\twitter\\\\*.csv'),\n",
    "    'clean':'C:\\\\Users\\\\Federico\\\\Downloads\\\\Clean',\n",
    "    'database':'C:\\\\Users\\\\Federico\\\\Downloads\\\\Clean\\\\news.db'\n",
    "}\n",
    "\n",
    "Path('C:\\\\Users\\\\Federico\\\\Downloads\\\\Clean').mkdir(parents=True, exist_ok=True)\n",
    "MONGO_CONNECTION_STRING = \"mongodb://socialnews:root@172.16.5.20:27017,172.16.5.21:27017,172.16.5.22:27017/socialNewsDB?authSource=admin&replicaSet=socialNews&readPreference=primary&appname=PythonLoadScript&ssl=false\"\n",
    "\n",
    "trusted_sources = [\n",
    "    '228735667216',\n",
    "    '15704546335',\n",
    "    '86680728811',\n",
    "    '155869377766434',\n",
    "    '131459315949',\n",
    "    '5550296508',\n",
    "    '6250307292',\n",
    "    '5281959998',\n",
    "    '6013004059',\n",
    "    '8860325749',\n",
    "    '10513336322',\n",
    "    '164305410295882',\n",
    "    '268914272540',\n",
    "    '18468761129',\n",
    "    '10606591490',\n",
    "    '7382473689',\n",
    "    '273864989376427',\n",
    "    '10643211755'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    reporters = list(client['socialNewsDB']['reporters'].find({'location':{'$ne':'Italy'}},{'_id':0, 'reporterId':1, 'fullName':1}))\n",
    "    readers = list(client['socialNewsDB']['users'].find({'isAdmin':{'$exists': False}},{'_id':1, 'fullName':1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = None\n",
    "for index, filename in enumerate(path['twitter']):\n",
    "    new_frame = pd.read_csv(filename,  dtype={'tweetid':str, 'quoted_status_id':str})\\\n",
    "        .query(\"verified and language.str.lower() == 'en'\")\n",
    "\n",
    "    if frame is None:\n",
    "        frame = new_frame.copy(deep=True)\n",
    "    else:\n",
    "        frame = pd.concat([frame, new_frame])\n",
    "frame.to_csv(f'{path[\"clean\"]}\\\\twitter_verified_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = pd.read_csv(f'{path[\"clean\"]}\\\\twitter_verified_en.csv')\n",
    "filtered_col = frame.loc[:,['tweetid', 'hashed_userid', 'tweetcreatedts','text']]\n",
    "filtered_col.to_csv(f'{path[\"clean\"]}\\\\tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_clean = pd.read_csv(f'{path[\"clean\"]}\\\\tweet.csv')\n",
    "hashtag = []\n",
    "links = []\n",
    "text_clean = []\n",
    "for index, row in twitter_clean.iterrows():\n",
    "    tweet = row['text']\n",
    "    hashtag.append(re.findall(\"#(\\w+)\", tweet))\n",
    "    links.append(re.findall(\"(https?://\\S+)\", tweet))\n",
    "\n",
    "    #for h in hashtag[-1]:\n",
    "    #    tweet = tweet.replace(f'#{h}', \"\")\n",
    "\n",
    "    for link in links[-1]:\n",
    "        tweet = tweet.replace(link, \"\")\n",
    "\n",
    "    text_clean.append(re.sub('@\\w*', '', tweet).strip(' :.,@\\n|'))\n",
    "\n",
    "twitter_clean['text']=text_clean\n",
    "twitter_clean['hashtags']=hashtag\n",
    "twitter_clean['links']=links\n",
    "\n",
    "twitter_clean.rename(columns={'tweetid':'_id', 'hashed_userid':'reporterId', 'tweetcreatedts':'timestamp'}, inplace=True)\n",
    "# twitter_clean.to_csv(f\"{path['clean']}\\\\twitter_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matching_reporters_tweet = {}\n",
    "\n",
    "if os.path.isfile(f'{path[\"clean\"]}\\\\matching_reporters_tweet.json'):\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters_tweet.json') as fin:\n",
    "        matching_reporters_tweet = json.load(fin)\n",
    "else:\n",
    "    index = 0\n",
    "    for old_reporter_id in twitter_clean['reporterId']:\n",
    "        matching_reporters_tweet[old_reporter_id] = reporters[index]\n",
    "        index = (index + 1) % len(reporters)\n",
    "\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters_tweet.json','w') as fout:\n",
    "        json.dump(matching_reporters_tweet, fout, indent=1)\n",
    "\n",
    "twitter_clean['reporterId'] = twitter_clean['reporterId'].map(lambda value: matching_reporters_tweet[value]['reporterId'])\n",
    "twitter_clean['_id'] = [str(uuid.uuid4()) for index in range(0, len(twitter_clean))]\n",
    "twitter_clean = twitter_clean.loc[:,['_id','reporterId','timestamp','text','hashtags','links']]\n",
    "with open(f'{path[\"clean\"]}\\\\twitter_posts_clean.json','w') as fout:\n",
    "    json.dump(twitter_clean.to_dict('records'), fout, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def callback(x):\n",
    "    del(x['reporterId'])\n",
    "    if len(x['hashtags']) == 0:\n",
    "        del(x['hashtags'])\n",
    "    if len(x['links']) == 0:\n",
    "        del(x['links'])\n",
    "    return x\n",
    "\n",
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        twitter_clean['timestamp'] = twitter_clean['timestamp'].map(lambda ts:  datetime.strptime(ts, '%Y-%m-%d %H:%M:%S'))\n",
    "        for reporter in reporters:\n",
    "            new_posts = twitter_clean.query(f\"reporterId == '{reporter['reporterId']}'\").to_dict('records')\n",
    "            new_posts=list(map(callback, new_posts))\n",
    "            if len(new_posts) == 0:\n",
    "                continue\n",
    "            res = client['socialNewsDB']['reporters'].update_one({'reporterId':reporter['reporterId'], 'email':{'$exists':True}}, {'$push':{'posts': {'$each': new_posts}}})\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = pd.read_csv(path['facebook']['post'], dtype={'page_id':str})\n",
    "comments = pd.read_csv(path['facebook']['comment'], dtype={'message':str})\n",
    "sources = pd.read_csv(path['facebook']['source'], index_col=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_filtered = posts.loc[:,['created_time', 'link', 'message', 'page_id', 'post_id']]\n",
    "posts_filtered.rename(columns={'post_id':'_id', 'page_id':'reporterId', 'message':'text', 'created_time':'timestamp', 'link':'links'}, inplace=True)\n",
    "\n",
    "posts_filtered['_id'] = posts_filtered['_id'].map(lambda value: value.split('_')[-1])\n",
    "posts_filtered = posts_filtered[posts_filtered['reporterId'].isin(trusted_sources)]\n",
    "posts_filtered = posts_filtered[posts_filtered['text'].notnull()]\n",
    "posts_filtered['links'] = posts_filtered['links'].map(lambda value: [value] if type(value) == type('') else [])\n",
    "posts_filtered['hashtags'] = posts_filtered['text'].map(lambda value: re.findall('#(\\w+)', value))\n",
    "posts_filtered['text'] = posts_filtered['text'].map(lambda value: re.sub(\"(https?://\\S+)|(#(\\w+))\",\"\", value).strip(' :.,@\\n|').replace(\"\\\"\", \"\"))\n",
    "\n",
    "for reporter, newId in zip(posts_filtered.drop_duplicates(['reporterId']).loc[:,'reporterId'], reporters):\n",
    "        posts_filtered.replace(reporter, newId['reporterId'], inplace=True)\n",
    "old_post_ids = posts_filtered['_id']\n",
    "posts_filtered.to_csv(f'{path[\"clean\"]}\\\\facebook_posts_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matching_reporters = {}\n",
    "\n",
    "if os.path.isfile(f'{path[\"clean\"]}\\\\matching_reporters.json'):\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters.json') as fin:\n",
    "        matching_reporters = json.load(fin)\n",
    "else:\n",
    "    for old_post_id, reporter_id in zip(posts_filtered['_id'], posts_filtered['reporterId']):\n",
    "        matching_reporters[old_post_id] = {'newId': str(uuid.uuid4()), 'reporterId': reporter_id}\n",
    "\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters.json','w') as fout:\n",
    "        json.dump(matching_reporters, fout, indent=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "fb_post_ids = [str(uuid.uuid4()) for _ in range(0,len(posts_filtered))]\n",
    "fb_post_reporters = [reporters[random.randint(0, len(reporters)-1)]['reporterId'] for _ in range(0,len(posts_filtered))]\n",
    "post_matching = {}\n",
    "\n",
    "for (old,newPostId, reporterId) in zip(posts_filtered['_id'], fb_post_ids, fb_post_reporters):\n",
    "    post_matching[old] = {}\n",
    "    post_matching[old]['postId'] = newPostId\n",
    "    post_matching[old]['reporterId'] = reporterId\n",
    "\n",
    "fb_post_ids_iter = iter(fb_post_ids)\n",
    "fb_post_reporters_iter = iter(fb_post_reporters)\n",
    "\n",
    "posts_filtered['_id'] = posts_filtered['_id'].map(lambda _: next(fb_post_ids_iter))\n",
    "posts_filtered['reporterId'] = posts_filtered['reporterId'].map(lambda _: next(fb_post_reporters_iter))\n",
    "                                                  \n",
    "posts_filtered['timestamp'] = posts_filtered['timestamp'].astype('str')\n",
    "with open(f'{path[\"clean\"]}\\\\facebook_posts_clean.json','w') as fout:\n",
    "    json.dump(posts_filtered.iloc[:,:].to_dict('records'), fout, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv(path['facebook']['comment'], dtype={'message':str})\n",
    "\n",
    "\n",
    "comments_filtered = comments.loc[:, ['from_id','created_time','message', 'post_name']]\n",
    "comments_filtered.rename(columns={'from_id':'reader', 'message':'text', 'created_time':'timestamp'}, inplace=True)\n",
    "\n",
    "comments_filtered['post'] = comments_filtered['post_name'].map(lambda value: {'_id': value.split('_')[-1], 'reporterId':''})\n",
    "# comments_filtered['reader'] = comments_filtered['reader'].map(lambda value: {'_id':value, 'fullName':''})\n",
    "comments_filtered = comments_filtered[comments_filtered['post'].str['_id'].isin(old_post_ids)]\n",
    "comments_filtered[\"_id\"] = [str(uuid.uuid4()) for index in range(0, len(comments_filtered))]\n",
    "comments_filtered = comments_filtered[comments_filtered['text'].notnull()]\n",
    "comments_filtered['text'] = comments_filtered['text'].map(lambda value: value.strip().replace(\"\\\"\", \"\"))\n",
    "comments_filtered = comments_filtered.loc[:, ['_id', 'reader', 'post', 'text', 'timestamp']]\n",
    "\n",
    "comments_filtered['post'] = comments_filtered['post'].map(lambda old: {'_id':post_matching[old['_id']]['postId'], 'reporterId': post_matching[old['_id']]['reporterId']})\n",
    "\n",
    "\n",
    "comments_filtered.to_csv(f'{path[\"clean\"]}\\\\facebook_comments_clean.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matching_readers = {}\n",
    "\n",
    "if os.path.isfile(f'{path[\"clean\"]}\\\\matching_readers.json'):\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_readers.json') as fin:\n",
    "        matching_readers = json.load(fin)\n",
    "else:\n",
    "    print(len(comments_filtered['reader']))\n",
    "    print(len(comments_filtered['reader'].drop_duplicates()))\n",
    "\n",
    "    index = 0\n",
    "    for old_reader_id in comments_filtered['reader']:\n",
    "        matching_readers[old_reader_id] = {'_id': readers[index]['_id'], 'fullName': readers[index]['fullName']}\n",
    "        index = (index + 1) % len(readers)\n",
    "\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_readers.json','w') as fout:\n",
    "        json.dump(matching_readers, fout, indent=1)\n",
    "\n",
    "\n",
    "comments_filtered['reader'] = comments_filtered['reader'].map(lambda value: matching_readers[value])\n",
    "\n",
    "with open(f'{path[\"clean\"]}\\\\facebook_comments_clean.json','w') as fout:\n",
    "    json.dump(comments_filtered.iloc[:,:].to_dict('records'), fout, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def callback(x):\n",
    "    del(x['reporterId'])\n",
    "    if len(x['hashtags']) == 0:\n",
    "        del(x['hashtags'])\n",
    "    if len(x['links']) == 0:\n",
    "        del(x['links'])\n",
    "    return x\n",
    "\n",
    "posts_filtered = pd.read_json(f'{path[\"clean\"]}\\\\facebook_posts_clean.json')\n",
    "\n",
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        #posts_filtered['timestamp'] = posts_filtered['timestamp'].map(lambda ts:  datetime.strptime(str(ts), '%Y-%m-%d %H:%M:%S'))\n",
    "        for reporter in reporters:\n",
    "            new_posts = posts_filtered.query(f\"reporterId == '{reporter['reporterId']}'\").to_dict('records')\n",
    "            new_posts=list(map(callback, new_posts))\n",
    "            if len(new_posts) == 0:\n",
    "                continue\n",
    "            res = client['socialNewsDB']['reporters'].update_one({'reporterId':reporter['reporterId'], 'email':{'$exists':True}}, {'$set':{'posts': new_posts}})\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: batch op errors occurred, full error: {'writeErrors': [{'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: socialNewsDB.comments index: _id_ dup key: { _id: \"31ceb9e2-c980-43ed-8b12-30ac75e2749e\" }', 'keyPattern': {'_id': 1}, 'keyValue': {'_id': '31ceb9e2-c980-43ed-8b12-30ac75e2749e'}, 'op': {'_id': '31ceb9e2-c980-43ed-8b12-30ac75e2749e', 'reader': {'_id': '0d81e649-28c8-42af-a7e6-5770ee4d068d', 'fullName': 'Eliza Hoftun'}, 'post': {'_id': 'c23d7e5a-2d10-44f0-b190-31db30f1d909', 'reporterId': '79eb5f8a-7619-49cd-a1b6-3a84cadab09e'}, 'text': \"We are speaking to NRA supporters as well as Women's March supporters\", 'timestamp': Timestamp('2017-07-14 14:43:54')}}], 'writeConcernErrors': [], 'nInserted': 0, 'nUpserted': 0, 'nMatched': 0, 'nModified': 0, 'nRemoved': 0, 'upserted': []}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def callback(x):\n",
    "    del(x['reporterId'])\n",
    "    if len(x['hashtags']) == 0:\n",
    "        del(x['hashtags'])\n",
    "    if len(x['links']) == 0:\n",
    "        del(x['links'])\n",
    "    return x\n",
    "\n",
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        comments_filtered['timestamp'] = comments_filtered['timestamp'].map(lambda ts:  datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S+%f'))\n",
    "        for reporter in reporters:\n",
    "            res = client['socialNewsDB']['comments'].insert_many(comments_filtered.to_dict('records'))\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        counters = client['socialNewsDB']['comments'].aggregate([{'$group':{'_id':'$post._id', 'nComment':{'$sum':1}}}])\n",
    "        for counter in counters:\n",
    "            client['socialNewsDB']['reporters'].update_one({'posts._id':counter['_id']},{'$set':{'posts.$.numOfComment':counter['nComment']}})\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "URI = \"neo4j://172.16.5.22:7687\"\n",
    "AUTH = (\"neo4j\", \"AdminPsw\")\n",
    "QUERY = f\"MATCH(r: Reporter) WHERE NOT r.reporterId IN {[reporter['reporterId'] for reporter in reporters]} DETACH DELETE r RETURN r\"\n",
    "\n",
    "def delete_reporter(tx):\n",
    "    res = tx.run(QUERY)\n",
    "    reslist =  [record.data() for record in res]\n",
    "    print(len(reslist))\n",
    "    \n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(delete_reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        cursor = client['socialNewsDB']['users'].aggregate([\n",
    "            {'$group':{'_id':'$email', 'count':{'$sum':1}}}, \n",
    "            {'$match':{'count':{'$gt':1}}}\n",
    "        ])\n",
    "        duplicates = list(cursor)\n",
    "        json.dump(duplicates, open('duplicates.json','w'), indent=2)\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try: \n",
    "        for dup in duplicates:\n",
    "            client['socialNewsDB']['users'].delete_one({'email':dup['_id']})\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119618\n"
     ]
    }
   ],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try: \n",
    "        cursor = client['socialNewsDB']['comments'].aggregate([{ '$group': { '_id': '$reader._id'}}])\n",
    "        readers = list(cursor)\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try: \n",
    "        cursor = client['socialNewsDB']['users'].find({},{'_id':1})\n",
    "        users = list(cursor)\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_list = []\n",
    "readers_list = []\n",
    "for user in users:\n",
    "    users_list.append(user['_id'])\n",
    "for reader in readers:\n",
    "    readers_list.append(reader['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "toRemove = list(set(readers_list) - set(users_list))\n",
    "print(len(toRemove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try: \n",
    "        for index, remove in enumerate(toRemove):\n",
    "            print(index)\n",
    "            client['socialNewsDB']['comments'].delete_many({'reader._id': remove})\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "8aaef33e4825cc75c10d5503a8badec4de06c5b884d15b4da0ae20ec31a339b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
