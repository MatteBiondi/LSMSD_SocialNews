{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "path = {\n",
    "    'facebook':{\n",
    "        'post':'..\\\\data\\\\facebook\\\\fb_news_posts_20K.csv',\n",
    "        'comment':'..\\\\data\\\\facebook\\\\fb_news_comments_1000K_hashed.csv',\n",
    "        'source':'..\\\\data\\\\facebook\\\\fb_news_pagenames.csv'\n",
    "        },\n",
    "    'twitter':glob.glob('..\\\\data\\\\twitter\\\\*.csv'),\n",
    "    'clean':'C:\\\\Users\\\\Federico\\\\Downloads\\\\Clean',\n",
    "    'database':'C:\\\\Users\\\\Federico\\\\Downloads\\\\Clean\\\\news.db'\n",
    "}\n",
    "\n",
    "Path('C:\\\\Users\\\\Federico\\\\Downloads\\\\Clean').mkdir(parents=True, exist_ok=True)\n",
    "MONGO_CONNECTION_STRING = \"mongodb://socialnews:root@172.16.5.20:27017,172.16.5.21:27017,172.16.5.22:27017/socialNewsDB?authSource=admin&replicaSet=socialNews&readPreference=primary&appname=PythonLoadScript&ssl=false\"\n",
    "\n",
    "trusted_sources = [\n",
    "    '228735667216',\n",
    "    '15704546335',\n",
    "    '86680728811',\n",
    "    '155869377766434',\n",
    "    '131459315949',\n",
    "    '5550296508',\n",
    "    '6250307292',\n",
    "    '5281959998',\n",
    "    '6013004059',\n",
    "    '8860325749',\n",
    "    '10513336322',\n",
    "    '164305410295882',\n",
    "    '268914272540',\n",
    "    '18468761129',\n",
    "    '10606591490',\n",
    "    '7382473689',\n",
    "    '273864989376427',\n",
    "    '10643211755'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    reporters = list(client['socialNewsDB']['reporters'].find({'location':{'$ne':'Italy'}},{'_id':0, 'reporterId':1, 'fullName':1}))\n",
    "    readers = list(client['socialNewsDB']['users'].find({'isAdmin':{'$exists': False}},{'_id':1, 'fullName':1}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Twitter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "frame = None\n",
    "for index, filename in enumerate(path['twitter']):\n",
    "    new_frame = pd.read_csv(filename,  dtype={'tweetid':str, 'quoted_status_id':str})\\\n",
    "        .query(\"verified and language.str.lower() == 'en'\")\n",
    "\n",
    "    if frame is None:\n",
    "        frame = new_frame.copy(deep=True)\n",
    "    else:\n",
    "        frame = pd.concat([frame, new_frame])\n",
    "frame.to_csv(f'{path[\"clean\"]}\\\\twitter_verified_en.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "frame = pd.read_csv(f'{path[\"clean\"]}\\\\twitter_verified_en.csv')\n",
    "filtered_col = frame.loc[:,['tweetid', 'hashed_userid', 'tweetcreatedts','text']]\n",
    "filtered_col.to_csv(f'{path[\"clean\"]}\\\\tweet.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "twitter_clean = pd.read_csv(f'{path[\"clean\"]}\\\\tweet.csv')\n",
    "hashtag = []\n",
    "links = []\n",
    "text_clean = []\n",
    "for index, row in twitter_clean.iterrows():\n",
    "    tweet = row['text']\n",
    "    hashtag.append(re.findall(\"#(\\w+)\", tweet))\n",
    "    links.append(re.findall(\"(https?://\\S+)\", tweet))\n",
    "\n",
    "    for h in hashtag[-1]:\n",
    "        tweet = tweet.replace(f'#{h}', \"\")\n",
    "\n",
    "    for link in links[-1]:\n",
    "        tweet = tweet.replace(link, \"\")\n",
    "\n",
    "    text_clean.append(re.sub('@\\w*', '', tweet).strip(' :.,@\\n|'))\n",
    "\n",
    "twitter_clean['text']=text_clean\n",
    "twitter_clean['hashtags']=hashtag\n",
    "twitter_clean['links']=links\n",
    "\n",
    "twitter_clean.rename(columns={'tweetid':'_id', 'hashed_userid':'reporterId', 'tweetcreatedts':'timestamp'}, inplace=True)\n",
    "# twitter_clean.to_csv(f\"{path['clean']}\\\\twitter_clean.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "matching_reporters_tweet = {}\n",
    "\n",
    "if os.path.isfile(f'{path[\"clean\"]}\\\\matching_reporters_tweet.json'):\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters_tweet.json') as fin:\n",
    "        matching_reporters_tweet = json.load(fin)\n",
    "else:\n",
    "    index = 0\n",
    "    for old_reporter_id in twitter_clean['reporterId']:\n",
    "        matching_reporters_tweet[old_reporter_id] = reporters[index]\n",
    "        index = (index + 1) % len(reporters)\n",
    "\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters_tweet.json','w') as fout:\n",
    "        json.dump(matching_reporters_tweet, fout, indent=1)\n",
    "\n",
    "twitter_clean['reporterId'] = twitter_clean['reporterId'].map(lambda value: matching_reporters_tweet[value]['reporterId'])\n",
    "twitter_clean['_id'] = [str(uuid.uuid4()) for index in range(0, len(twitter_clean))]\n",
    "twitter_clean = twitter_clean.loc[:,['_id','reporterId','timestamp','text','hashtags','links']]\n",
    "with open(f'{path[\"clean\"]}\\\\twitter_posts_clean.json','w') as fout:\n",
    "    json.dump(twitter_clean.to_dict('records'), fout, indent=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def callback(x):\n",
    "    del(x['reporterId'])\n",
    "    if len(x['hashtags']) == 0:\n",
    "        del(x['hashtags'])\n",
    "    if len(x['links']) == 0:\n",
    "        del(x['links'])\n",
    "    return x\n",
    "\n",
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        twitter_clean['timestamp'] = twitter_clean['timestamp'].map(lambda ts:  datetime.strptime(ts, '%Y-%m-%d %H:%M:%S'))\n",
    "        for reporter in reporters:\n",
    "            new_posts = twitter_clean.query(f\"reporterId == '{reporter['reporterId']}'\").to_dict('records')\n",
    "            new_posts=list(map(callback, new_posts))\n",
    "            if len(new_posts) == 0:\n",
    "                continue\n",
    "            res = client['socialNewsDB']['reporters'].update_one({'reporterId':reporter['reporterId'], 'email':{'$exists':True}}, {'$push':{'posts': {'$each': new_posts}}})\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Facebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "posts = pd.read_csv(path['facebook']['post'], dtype={'page_id':str})\n",
    "comments = pd.read_csv(path['facebook']['comment'], dtype={'message':str})\n",
    "sources = pd.read_csv(path['facebook']['source'], index_col=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "posts_filtered = posts.loc[:,['created_time', 'link', 'message', 'page_id', 'post_id']]\n",
    "posts_filtered.rename(columns={'post_id':'_id', 'page_id':'reporterId', 'message':'text', 'created_time':'timestamp', 'link':'links'}, inplace=True)\n",
    "\n",
    "posts_filtered['_id'] = posts_filtered['_id'].map(lambda value: value.split('_')[-1])\n",
    "posts_filtered = posts_filtered[posts_filtered['reporterId'].isin(trusted_sources)]\n",
    "posts_filtered = posts_filtered[posts_filtered['text'].notnull()]\n",
    "posts_filtered['links'] = posts_filtered['links'].map(lambda value: [value] if type(value) == type('') else [])\n",
    "posts_filtered['hashtags'] = posts_filtered['text'].map(lambda value: re.findall('#(\\w+)', value))\n",
    "posts_filtered['text'] = posts_filtered['text'].map(lambda value: re.sub(\"(https?://\\S+)|(#(\\w+))\",\"\", value).strip(' :.,@\\n|').replace(\"\\\"\", \"\"))\n",
    "\n",
    "for reporter, newId in zip(posts_filtered.drop_duplicates(['reporterId']).loc[:,'reporterId'], reporters):\n",
    "        posts_filtered.replace(reporter, newId['reporterId'], inplace=True)\n",
    "old_post_ids = posts_filtered['_id']\n",
    "posts_filtered.to_csv(f'{path[\"clean\"]}\\\\facebook_posts_filtered.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "matching_reporters = {}\n",
    "\n",
    "if os.path.isfile(f'{path[\"clean\"]}\\\\matching_reporters.json'):\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters.json') as fin:\n",
    "        matching_reporters = json.load(fin)\n",
    "else:\n",
    "    for old_post_id, reporter_id in zip(posts_filtered['_id'], posts_filtered['reporterId']):\n",
    "        matching_reporters[old_post_id] = {'newId': str(uuid.uuid4()), 'reporterId': reporter_id}\n",
    "\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_reporters.json','w') as fout:\n",
    "        json.dump(matching_reporters, fout, indent=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "posts_filtered['_id'] = posts_filtered['_id'].map(lambda old: matching_reporters[old]['newId'])\n",
    "with open(f'{path[\"clean\"]}\\\\facebook_posts_clean.json','w') as fout:\n",
    "    json.dump(posts_filtered.iloc[:,:].to_dict('records'), fout, indent=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "comments_filtered = comments.loc[:, ['from_id','created_time','message', 'post_name']]\n",
    "comments_filtered.rename(columns={'from_id':'reader', 'message':'text', 'created_time':'timestamp'}, inplace=True)\n",
    "\n",
    "comments_filtered['post'] = comments_filtered['post_name'].map(lambda value: {'_id': value.split('_')[-1], 'reporterId':''})\n",
    "# comments_filtered['reader'] = comments_filtered['reader'].map(lambda value: {'_id':value, 'fullName':''})\n",
    "comments_filtered = comments_filtered[comments_filtered['post'].str['_id'].isin(old_post_ids)]\n",
    "comments_filtered[\"_id\"] = [str(uuid.uuid4()) for index in range(0, len(comments_filtered))]\n",
    "comments_filtered = comments_filtered[comments_filtered['text'].notnull()]\n",
    "comments_filtered['text'] = comments_filtered['text'].map(lambda value: value.strip().replace(\"\\\"\", \"\"))\n",
    "comments_filtered = comments_filtered.loc[:, ['_id', 'reader', 'post', 'text', 'timestamp']]\n",
    "\n",
    "comments_filtered.to_csv(f'{path[\"clean\"]}\\\\facebook_comments_clean.csv')\n",
    "\n",
    "comments_filtered['post'] = comments_filtered['post'].map(lambda old: {'_id':matching_reporters[old['_id']]['newId'], 'reporterId': matching_reporters[old['_id']]['reporterId']})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "matching_readers = {}\n",
    "\n",
    "if os.path.isfile(f'{path[\"clean\"]}\\\\matching_readers.json'):\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_readers.json') as fin:\n",
    "        matching_readers = json.load(fin)\n",
    "else:\n",
    "    print(len(comments_filtered['reader']))\n",
    "    print(len(comments_filtered['reader'].drop_duplicates()))\n",
    "\n",
    "    index = 0\n",
    "    for old_reader_id in comments_filtered['reader']:\n",
    "        matching_readers[old_reader_id] = {'_id': readers[index]['_id'], 'fullName': readers[index]['fullName']}\n",
    "        index = (index + 1) % len(readers)\n",
    "\n",
    "    with open(f'{path[\"clean\"]}\\\\matching_readers.json','w') as fout:\n",
    "        json.dump(matching_readers, fout, indent=1)\n",
    "\n",
    "\n",
    "comments_filtered['reader'] = comments_filtered['reader'].map(lambda value: matching_readers[value])\n",
    "\n",
    "with open(f'{path[\"clean\"]}\\\\facebook_comments_clean.json','w') as fout:\n",
    "    json.dump(comments_filtered.iloc[:,:].to_dict('records'), fout, indent=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import to DB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def callback(x):\n",
    "    del(x['reporterId'])\n",
    "    if len(x['hashtags']) == 0:\n",
    "        del(x['hashtags'])\n",
    "    if len(x['links']) == 0:\n",
    "        del(x['links'])\n",
    "    return x\n",
    "\n",
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        posts_filtered['timestamp'] = posts_filtered['timestamp'].map(lambda ts:  datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S+%f'))\n",
    "        for reporter in reporters:\n",
    "            new_posts = posts_filtered.query(f\"reporterId == '{reporter['reporterId']}'\").to_dict('records')\n",
    "            new_posts=list(map(callback, new_posts))\n",
    "            if len(new_posts) == 0:\n",
    "                continue\n",
    "            res = client['socialNewsDB']['reporters'].update_one({'reporterId':reporter['reporterId'], 'email':{'$exists':True}}, {'$set':{'posts': new_posts}})\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def callback(x):\n",
    "    del(x['reporterId'])\n",
    "    if len(x['hashtags']) == 0:\n",
    "        del(x['hashtags'])\n",
    "    if len(x['links']) == 0:\n",
    "        del(x['links'])\n",
    "    return x\n",
    "\n",
    "with MongoClient(MONGO_CONNECTION_STRING) as client:\n",
    "    try:\n",
    "        comments_filtered['timestamp'] = comments_filtered['timestamp'].map(lambda ts:  datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S+%f'))\n",
    "        for reporter in reporters:\n",
    "            res = client['socialNewsDB']['comments'].insert_many(comments_filtered.to_dict('records'))\n",
    "    except Exception as ex:\n",
    "        print(f'Error: {ex}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}