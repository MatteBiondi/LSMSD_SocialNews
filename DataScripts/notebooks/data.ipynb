{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "path = {\n",
    "    'facebook':{\n",
    "        'post':'..\\\\data\\\\facebook\\\\fb_news_posts_20K.csv',\n",
    "        'comment':'..\\\\data\\\\facebook\\\\fb_news_comments_1000K_hashed.csv',\n",
    "        'source':'..\\\\data\\\\facebook\\\\fb_news_pagenames.csv'\n",
    "        },\n",
    "    'twitter':glob.glob('..\\\\data\\\\twitter\\\\*.csv'),\n",
    "    'clean':'..\\\\data\\\\clean',\n",
    "    'database':'..\\\\database\\\\news.db'\n",
    "}\n",
    "\n",
    "Path('..\\\\data\\\\clean').mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Twitter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frame = None\n",
    "for index, filename in enumerate(path['twitter']):\n",
    "    new_frame = pd.read_csv(filename, index_col=9, dtype={'tweetid':str, 'quoted_status_id':str})\\\n",
    "        .query(\"verified and language.str.lower() == 'en'\")\n",
    "\n",
    "    if frame is None:\n",
    "        frame = new_frame.copy(deep=True)\n",
    "    else:\n",
    "        frame = pd.concat([frame, new_frame])\n",
    "frame.to_csv(f'{path[\"clean\"]}\\\\twitter_verified_en.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "frame = pd.read_csv(f'{path[\"clean\"]}\\\\twitter_verified_en.csv')\n",
    "filtered_col = frame.loc[:,['tweetid', 'hashed_userid', 'tweetcreatedts','text']]\n",
    "filtered_col.to_csv(f'{path[\"clean\"]}\\\\tweet_clean.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with sqlite3.connect(path['database']) as conn:\n",
    "    filtered_col.to_sql('twitter', conn, if_exists='replace')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "clean = pd.read_csv(f'{path[\"clean\"]}\\\\tweet_clean.csv')\n",
    "hashtag = []\n",
    "links = []\n",
    "text_clean = []\n",
    "for index, row in clean.iterrows():\n",
    "    tweet = row['text']\n",
    "    hashtag.append(re.findall(\"#(\\w+)\", tweet))\n",
    "    links.append(re.findall(\"(https?://\\S+)\", tweet))\n",
    "\n",
    "    for h in hashtag[-1]:\n",
    "        tweet = tweet.replace(f'#{h}', \"\")\n",
    "\n",
    "    for link in links[-1]:\n",
    "        tweet = tweet.replace(link, \"\")\n",
    "\n",
    "    text_clean.append(tweet.strip(' :.,@\\n|'))\n",
    "\n",
    "clean['text']=text_clean\n",
    "clean['hashtag']=hashtag\n",
    "clean['link']=links\n",
    "clean.index = clean['tweetid']\n",
    "clean.iloc[:,2:].to_csv(f\"{path['clean']}\\\\twitter_clean.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "clean.iloc[:,2:].to_csv(f\"{path['clean']}\\\\twitter_clean.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Facebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "posts = pd.read_csv(path['facebook']['post'])\n",
    "comments = pd.read_csv(path['facebook']['comment'], index_col=[0,1])\n",
    "sources = pd.read_csv(path['facebook']['source'], index_col=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "posts_filtered = posts.loc[:,['created_time', 'link', 'message', 'page_id', 'post_id']]\n",
    "posts_filtered['post_id'] = posts_filtered['post_id'].apply(lambda value: value.split('_')[-1])\n",
    "posts_filtered.index = posts_filtered['post_id']\n",
    "posts_filtered = posts_filtered.iloc[:,:-1]\n",
    "posts_filtered.to_csv(f'{path[\"clean\"]}\\\\facebook_posts.csv')\n",
    "\n",
    "comments_filtered = comments.loc[:, ['message', 'post_name']]\n",
    "comments_filtered['post_id'] = comments_filtered['post_name'].apply(lambda value: value.split('_')[-1])\n",
    "comments_filtered.to_csv(f'{path[\"clean\"]}\\\\facebook_comments.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with sqlite3.connect(path['database']) as conn:\n",
    "    posts_filtered.to_sql('post', conn, if_exists='replace')\n",
    "    comments_filtered.to_sql('comment', conn, if_exists='replace')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dict_factory(cursor, row):\n",
    "    d = {}\n",
    "    for idx, col in enumerate(cursor.description):\n",
    "        d[col[0]] = row[idx]\n",
    "    return d\n",
    "\n",
    "trust_sources = [\n",
    "    '228735667216',\n",
    "    '15704546335',\n",
    "    '86680728811',\n",
    "    '155869377766434',\n",
    "    '131459315949',\n",
    "    '5550296508',\n",
    "    '6250307292',\n",
    "    '5281959998',\n",
    "    '6013004059',\n",
    "    '8860325749',\n",
    "    '10513336322',\n",
    "    '164305410295882',\n",
    "    '268914272540',\n",
    "    '18468761129',\n",
    "    '10606591490',\n",
    "    '7382473689',\n",
    "    '273864989376427',\n",
    "    '10643211755'\n",
    "]\n",
    "with sqlite3.connect(path['database']) as conn:\n",
    "    conn.row_factory = dict_factory\n",
    "    cur = conn.cursor()\n",
    "    comments_clean = cur.execute(f\"\"\"\n",
    "        select c.* from comment c inner join post p on c.post_id = p.post_id where p.page_id in(\n",
    "        {','.join(trust_sources)}\n",
    "        );\n",
    "    \"\"\").fetchall()\n",
    "    posts_clean = cur.execute(f\"\"\"\n",
    "        select p.* from post p where p.page_id in(\n",
    "        {','.join(trust_sources)}\n",
    "        );\n",
    "    \"\"\").fetchall()\n",
    "\n",
    "comments_clean = pd.DataFrame(comments_clean).loc[:, ['from_id', 'post_id', 'message', 'created_time']]\n",
    "posts_clean = pd.DataFrame(posts_clean)\n",
    "\n",
    "comments_clean.rename(columns={'from_id':'user_id', 'message':'text', 'created_time':'creation_timestamp'}, inplace=True)\n",
    "comments_clean.index = comments_clean['creation_timestamp']\n",
    "comments_clean = comments_clean.iloc[:, :-1]\n",
    "comments_clean.to_csv(f'{path[\"clean\"]}\\\\facebook_comments_clean.csv')\n",
    "\n",
    "posts_clean = posts_clean.loc[:,['post_id', 'page_id', 'message', 'created_time', 'link']]\n",
    "\n",
    "posts_clean.rename(columns={'page_id':'reporter_id', 'message':'text', 'created_time':'creation_timestamp'}, inplace=True)\n",
    "posts_clean.index = posts_clean['post_id']\n",
    "posts_clean = posts_clean.iloc[:, 1:]\n",
    "posts_clean.to_csv(f'{path[\"clean\"]}\\\\facebook_posts_clean.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
